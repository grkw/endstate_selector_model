<img width="320" alt="primitive-expansion" src="https://github.com/user-attachments/assets/2614c811-065e-4411-96e1-ee7a86d41380"></img>

# Endstate Selector

As an undergraduate research assistant in the UCLA [Verifiable & Control-Theoretic Robotics Laboratory (VECTR)](https://github.com/vectr-ucla), I contributed to a novel motion planning strategy to intelligently sample the state space using motion primitives when given a coarse reference path generated by a discrete graph-based search algorithm. While my work did not end up in the final [paper](https://arxiv.org/abs/2412.21180), it contributed to the development of the planner.

By exploiting key information inherent to the reference path, the proposed planner can strategically sample motion primitives in regions known to make progress to the goal, leading to significant improvements in computation time and robustness in trajectory generation. 
Additionally, this framework allows for sampling in a higher dimensional state space, i.e. acceleration, which contributes to smooth kinodynamic path plans.
However, the number of primitives that the planner must generate grows exponentially with the number of path waypoints and the number of state samplings. Thus, the planner quickly becomes infeasible for online deployment. The first half of my thesis presents an in-depth analysis of selected primitives and several algorithms for benchmarking performance (greedy, greedy lookahead, and random). The second half presents initial results using a fully-connected neural network in the behavior cloning (supervised learning) paradigm.

You can browse my [final report](https://github.com/user-attachments/files/18594075/Grace_Kwak_ECE181D_Report.pdf) and [research update slides](https://docs.google.com/presentation/d/1gLX9QPfOuHTSQCtCISJjY7ruSpFQj6g2e9HEk__Z6Ig/edit?usp=sharing).

## Contributions

The majority of my code contributions are in a private repo on the VECTR lab account. My capstone thesis makes the following contributions:
- Implementation and analysis of greedy, greedy-lookahead, and random state sampling strategies
- An in-depth analysis of selected state samples
- Infrastructure for learning-enabled state sampling features
- A simple fully-connected neural network for state sampling, trained using the behavior cloning paradigm of imitation learning (this repo)

## Architecture

- 2 Fully-connected hidden layers (including Batch normalization, ReLU activation, Dropout regularization)
- Softmax output layer

<img width="640" alt="nn_IO" src="https://github.com/user-attachments/assets/185453d2-efbe-4b90-9ef7-ad65536a018a" />

## Initial results

With 37 different classification options, random guessing accuracy would yield a 2.7% accuracy rate. After training on a small dataset with ~100 training examples per class,  I attained a test accuracy of 8.0%. Although this number is far below 100%, it shows that learning is possible.

<img width="640" alt="train_loss" src="https://github.com/user-attachments/assets/1442e713-aa0e-4cc8-b53f-06c516792801" />

## Acknowledgements

- My PhD student mentor [Helene J. Levy](https://github.com/hjlevy)
- [Professor Brett T. Lopez](https://github.com/btlopez) 
